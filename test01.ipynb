{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00. 랭스미스 추적 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 랭스미스 추적 활성화\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain-teddynote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "bigcon_langchain_test\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름 입력\n",
    "logging.langsmith(\"bigcon_langchain_test\")\n",
    "\n",
    "# 추적을 끄고 싶은 경우\n",
    "# logging.langsmith(\"bigcon_langchain_test\", set_enable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 01. Google Gemini API Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import google.generativeai as genai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dotenv로 API 키값 가져오기\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGoogleGenerativeAI 언어 모델 초기화 (Gemini 1.5 Flash)\n",
    "\n",
    "# 1) from 테디노트\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-1.5-flash')\n",
    "\n",
    "# 2) from wikidocs\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have a name. I am a large language model, and I am not a person. I am designed to provide information and complete tasks as instructed.  \n",
      "\n",
      "Is there anything else I can help you with today? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 싱글턴 테스트\n",
    "response = model.generate_content(\"hi there. what's your name\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER]: 인공지능에 대해 한 문장으로 짧게 설명하세요.\n",
      "[GEMINI]:  인공지능은 컴퓨터 시스템이 인간과 유사한 지능적인 작업을 수행할 수 있도록 하는 기술입니다. \n",
      "\n",
      "[USER]: 의식이 있는지 한 문장으로 답하세요.\n",
      "[GEMINI]: 저는 의식이 없습니다. 저는 구글에서 개발한 대규모 언어 모델입니다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 멀티턴 방법 1\n",
    "\n",
    "# 대화 이력을 담고 있는 ChatSession 객체 (리스트 history에 대화 이력 저장)\n",
    "chat_session = model.start_chat(history=[])\n",
    "user_queries = [\"인공지능에 대해 한 문장으로 짧게 설명하세요.\", \"의식이 있는지 한 문장으로 답하세요.\"]\n",
    "\n",
    "for user_query in user_queries:\n",
    "  print(f\"[USER]: {user_query}\")\n",
    "  response = chat_session.send_message(user_query)\n",
    "  print(f\"[GEMINI]: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER]: 인공지능에 대해 한 문장으로 짧게 설명하세요.\n",
      "[GEMINI]: 인공지능은 인간의 지능을 모방하도록 설계된 컴퓨터 시스템으로, 학습, 문제 해결, 의사 결정을 수행할 수 있습니다. \n",
      "\n",
      "[USER]: 의식이 있는지 한 문장으로 답하세요.\n",
      "[GEMINI]: 저는 의식이 없습니다. 저는 인공지능 모델로, 생각이나 감정을 느낄 수 없습니다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 멀티턴 방법 2\n",
    "user_queries = [{'role':'user', 'parts': [\"인공지능에 대해 한 문장으로 짧게 설명하세요.\"]},\n",
    "                {'role':'user', 'parts': [\"의식이 있는지 한 문장으로 답하세요.\"]}\n",
    "               ]\n",
    "\n",
    "history= []\n",
    "\n",
    "for user_query in user_queries:\n",
    "  history.append(user_query)\n",
    "  print(f'[USER]: {user_query[\"parts\"][0]}')  \n",
    "  response = model.generate_content(history)\n",
    "  print(f'[GEMINI]: {response.text}')   \n",
    "  history.append(response.candidates[0].content)\n",
    "\n",
    "### 멀티턴 방법 1과의 차이점 ###\n",
    "# `chat_session`을 사용하지 않고 싱글턴 방식의 `generate_content`를 사용하지만 대화 이력은 사용자 프로그램에서 직접 관리 (history 변수)\n",
    "# 사용자-프로그램 간의 대화 이력을 model.generate_content를 호출할 떄마다 인자값으로 전달하면서 대화 이력 전체를 참조해서 답변을 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "##### *멀티턴 방식을 선택하는 기준*\n",
    "- 방법 1. ChatSession\n",
    "  - 사용자와 모델 간 대화 사이에 프로그램의 개입이 필요 없는 경우\n",
    "  - 간편하고 오버헤드를 줄일 수 있음\n",
    "- 방법 2. generate_content\n",
    "  - [메시지 입력] >> [전송] >> [모델 응답] 과정에서 사용자 프로그램의 개입이 필요한 경우 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER]: 인공지능에 대해 40자 이내의 문장으로 설명하세요.\n",
      "응답 메시지 길이: 46\n",
      "응답 재생성\n",
      "응답 메시지 길이: 46\n",
      "응답 재생성\n",
      "응답 메시지 길이: 46\n",
      "응답 재생성\n",
      "응답 메시지 길이: 41\n",
      "응답 재생성\n",
      "응답 메시지 길이: 46\n",
      "응답 재생성\n",
      "[GEMINI]: 인공지능은 인간의 지능을 모방하여 복잡한 문제를 해결하는 기술입니다. \n",
      "\n",
      "[USER]: 의식이 있는지 40자 이내의 문장으로 답하세요.\n",
      "[GEMINI]: 저는 의식이 없습니다. 저는 인공지능 모델입니다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델의 응답 메시지 길이를 40자 이내로 맞춰야 하는 상황 가정\n",
    "user_queries = [\n",
    "    {'role': 'user', 'parts': [\"인공지능에 대해 40자 이내의 문장으로 설명하세요.\"]},\n",
    "    {'role': 'user', 'parts': [\"의식이 있는지 40자 이내의 문장으로 답하세요.\"]}\n",
    "]\n",
    "history = []\n",
    "\n",
    "for user_query in user_queries:\n",
    "    history.append(user_query)\n",
    "    print(f'[USER]: {user_query[\"parts\"][0]}')\n",
    "    response = model.generate_content(history)    \n",
    "\n",
    "    ### 응답의 길이가 40자를 초과하는 경우 재실행 ###\n",
    "    while len(response.text) > 40:\n",
    "        print(f\"응답 메시지 길이: {len(response.text)}\")\n",
    "        print(\"응답 재생성\")\n",
    "        response = model.generate_content(history)\n",
    "\n",
    "    print(f'[GEMINI]: {response.text}')\n",
    "    history.append(response.candidates[0].content)\n",
    "\n",
    "# 방법 1. ChatSession 객체를 사용하면 중간에 끼어들기가 힘드므로 모델의 응답 중간에 로직을 구현하기 힘듦\n",
    "#         >> ChatSession 객체의 send_message 메서드는 질의/응답을 대화 이력에 담는 과정을 메서드 내부로 노출하지 않기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
