{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Google Gemini 언어모델 제어 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# dotenv로 API 키값 가져오기\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "GEMINI_MODEL = os.getenv('GEMINI_MODEL')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 매개변수 설정\n",
    "- 언어모델의 동작은 <학습 시점>과 <실행 시점> 두 단계에 의해 결정됨\n",
    "  - 학습 시점: 가중치를 업데이트하는 방식으로 모델의 물리적 실체 완성\n",
    "  - 추론 시점(실행 시점): 만들어진 모델의 출력값을 조정함으로써 언어모델이 다양하게 반응하도록 도움\n",
    "\n",
    "- Google Gemini API에서 매개변수는 `GenerationConfig` 객체를 통해 설정됨\n",
    "  - 모델의 응답 수인 `candidate_count`를 포함해 총 6개의 매개변수가 있음 \n",
    "\n",
    "<br> \n",
    "\n",
    "![Google Gemini API 매개변수](https://wikidocs.net/images/page/229810/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7_2024-02-04_093623.png)\n",
    "\n",
    "*참고) 업데이트되는 매개변수를 **파라미터(parameter)** 라고 하고, 추론 시점에 사용되는 매개변수를 **인퍼런스 파라미터(inference parameter)** 로 구분지어 부르기도 함*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "##### *1) candidate_count*\n",
    "- 응답 후보(Candidate) 수를 설정하는 매개변수\n",
    "- API에서는 기본값인 1만 허용되므로 1이 아닌 다른 값을 설정한다면 오류 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "400 Multiple candidates is not enabled for models/gemini-1.5-flash",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\workspaces\\llmGeminiTest\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:65\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\workspaces\\llmGeminiTest\\.venv\\Lib\\site-packages\\grpc\\_channel.py:1181\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1175\u001b[0m (\n\u001b[0;32m   1176\u001b[0m     state,\n\u001b[0;32m   1177\u001b[0m     call,\n\u001b[0;32m   1178\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[0;32m   1179\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[0;32m   1180\u001b[0m )\n\u001b[1;32m-> 1181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\workspaces\\llmGeminiTest\\.venv\\Lib\\site-packages\\grpc\\_channel.py:1006\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[1;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[1;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"Multiple candidates is not enabled for models/gemini-1.5-flash\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.250.198.170:443 {created_time:\"2024-10-19T08:52:25.4519401+00:00\", grpc_status:3, grpc_message:\"Multiple candidates is not enabled for models/gemini-1.5-flash\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m generation_config \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mGenerationConfig(candidate_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mGenerativeModel(GEMINI_MODEL, generation_config\u001b[38;5;241m=\u001b[39mgeneration_config)\n\u001b[1;32m----> 4\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m인공지능에 대해 한 문장으로 설명하세요\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate 생성 건수: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(response\u001b[38;5;241m.\u001b[39mcandidates)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\workspaces\\llmGeminiTest\\.venv\\Lib\\site-packages\\google\\generativeai\\generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[1;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\workspaces\\llmGeminiTest\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:830\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m    829\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 830\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\workspaces\\llmGeminiTest\\.venv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:113\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m     metadata\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata)\n\u001b[0;32m    111\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metadata\n\u001b[1;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\workspaces\\llmGeminiTest\\.venv\\Lib\\site-packages\\google\\api_core\\retry.py:349\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    346\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    348\u001b[0m )\n\u001b[1;32m--> 349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\workspaces\\llmGeminiTest\\.venv\\Lib\\site-packages\\google\\api_core\\retry.py:191\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 191\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\workspaces\\llmGeminiTest\\.venv\\Lib\\site-packages\\google\\api_core\\timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\workspaces\\llmGeminiTest\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:67\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgument\u001b[0m: 400 Multiple candidates is not enabled for models/gemini-1.5-flash"
     ]
    }
   ],
   "source": [
    "generation_config = genai.GenerationConfig(candidate_count=2)  # 2로 설정할 경우 오류 발생\n",
    "model = genai.GenerativeModel(GEMINI_MODEL, generation_config=generation_config)\n",
    "\n",
    "response = model.generate_content(\"인공지능에 대해 한 문장으로 설명하세요\")\n",
    "print(response.text)\n",
    "print(f\"candidate 생성 건수: {len(response.candidates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "##### *2) stop_sequences*\n",
    "- 언어를 생성하다가 `stop_sequences`에 있는 문자열을 만나면 생성을 중단함\n",
    "- 민감한 어휘의 등장을 막거나, 응답 길이를 제한할 때 유용하게 사용 가능\n",
    "- 최대 5개까지 설정 가능\n",
    "    - 초과 시 InvalidARgument 오류 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 인공지능: 인간 지능을 모방하는 컴퓨터 시스템\n",
      "\n",
      "인공지능(AI)은 **컴퓨터 시스템이 인간과 유사한 지능을 보이도록 설계된 분야**를 말합니다\n"
     ]
    }
   ],
   "source": [
    "# 마침표나 느낌표가 등장하면 언어 생성을 중지\n",
    "generation_config = genai.GenerationConfig(stop_sequences=[\".\", \"!\"])\n",
    "model = genai.GenerativeModel(GEMINI_MODEL, generation_config=generation_config)\n",
    "\n",
    "response = model.generate_content(\"인공지능에 대해 설명하세요\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "##### *3) max_output_tokens*\n",
    "- 모델이 생성하는 메시지가 최대 토큰 수를 넘지 않도록 제어하는 매개변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_tokens: 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 토큰 확인해보기\n",
    "tokens = model.count_tokens(\"learn about language model tokenization.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates {\n",
      "  content {\n",
      "    parts {\n",
      "      text: \"\\354\\235\\270\\352\\263\\265\\354\\247\\200\\353\\212\\245\\354\\235\\200 \\354\\273\\264\\355\\223\\250\\355\\204\\260\\352\\260\\200\"\n",
      "    }\n",
      "    role: \"model\"\n",
      "  }\n",
      "  finish_reason: MAX_TOKENS\n",
      "  index: 0\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HATE_SPEECH\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HARASSMENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "}\n",
      "usage_metadata {\n",
      "  prompt_token_count: 14\n",
      "  candidates_token_count: 10\n",
      "  total_token_count: 24\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 최대 토큰 수를 10으로 제한\n",
    "generation_config = genai.GenerationConfig(max_output_tokens=10)\n",
    "\n",
    "model = genai.GenerativeModel(GEMINI_MODEL, generation_config=generation_config)\n",
    "user_message = \"인공지능에 대해 한 문장으로 설명하세요.\"\n",
    "response = model.generate_content(user_message)\n",
    "print(response._result)\n",
    "\n",
    "# finish_reason이 MAX_TOKENS인 것 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "##### *4) temperature*\n",
    "- `temperature`를 높게 설정하면 모델이 생성하는 언어의 예측 가능성은 떨어지고 독창성은 올라감\n",
    "- `temperature`가 낮아지면 안정적이면서도 일관된 답변을 생성함\n",
    "- 인공지능이 다음 낱말을 생성할 때 단어(token) 사전을 기준으로 확률분포를 만들기 때문에 매개변수로 언어모델의 독창성/일관성을 제어할 수 있음\n",
    "- `temperature`를 극단적으로 설정한 경우\n",
    "  - e.g. `temperature=100`: 단어들 사이의 차이가 사라짐 >> 말이 되지 않는 문장 생성\n",
    "  - e.g. `temperature=0.01`: 높은 값을 가진 한 단어만 남음 >> 매번 똑같은 문장 생성 \n",
    "- Gemini는 `temperature`를 0에서 2까지 설정할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "temperature=0:\n",
      "==================================================\n",
      "흰 눈 내려 쌓인 밤, \n",
      "차가운 바람 속 겨울. \n",
      "고요히 잠든 세상. \n",
      "\n",
      "==================================================\n",
      "흰 눈 내려 쌓인 밤, \n",
      "차가운 바람 속 겨울. \n",
      "고요히 잠든 세상. \n",
      "\n",
      "==================================================\n",
      "흰 눈 내려 쌓인 밤, \n",
      "차가운 바람 속 겨울. \n",
      "고요히 잠든 세상. \n",
      "\n",
      "\n",
      "temperature=1:\n",
      "==================================================\n",
      "흰 눈 내려 쌓이고, \n",
      "겨울밤은 깊어지네. \n",
      "고요히 잠든 세상. \n",
      "\n",
      "==================================================\n",
      "흰 눈 내려 쌓이니, \n",
      "세상은 잠든 듯. \n",
      "고요한 겨울밤. \n",
      "\n",
      "==================================================\n",
      "흰 눈 내려 쌓이고, \n",
      "세상은 잠들었다. \n",
      "겨울 밤, 고요히. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# temperature를 0과 1로 설정하고 3회 반복 수행\n",
    "model = genai.GenerativeModel(GEMINI_MODEL)\n",
    "user_message = \"겨울에 대한 짧은 시를 20자 이내로 지으세요.\"\n",
    "\n",
    "# temperature 0\n",
    "print(\"\\ntemperature=0:\")\n",
    "generation_config = genai.GenerationConfig(temperature=0)\n",
    "for _ in range(3):\n",
    "    response = model.generate_content(user_message , generation_config=generation_config)\n",
    "    print(f'{\"=\"*50}\\n{response.text}')\n",
    "\n",
    "# temperature 1\n",
    "print(\"\\ntemperature=1:\")\n",
    "generation_config = genai.GenerationConfig(temperature=1)\n",
    "for _ in range(3):\n",
    "    response = model.generate_content(user_message , generation_config=generation_config)\n",
    "    print(f'{\"=\"*50}\\n{response.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "##### *5) top_p*\n",
    "- 확률분포 내에서 선택할 단어의 범위를 결정하는 매개변수\n",
    "- 확률 역순으로 단어를 정렬한 후, 그 순서대로 단어를 선택해 가다가 누적 확률이 top_p에 도달하는 순간 선택을 멈추는 방식으로 동작\n",
    "- e.g.\n",
    "  - ![top_p](https://wikidocs.net/images/page/229816/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7_2024-02-04_093948.png)\n",
    "    - `temperature = 0.25, top_p=0.6`으로 설정했다면 \"오른다\"와 \"간다\"를 누적하는 순간 0.6에 도달 >> \"왔다\" 이후의 단어들은 선택에서 제외\n",
    "    - 선택된 두 개의 단어는 확률분포로 다시 만들어지고, 두 개의 확률분포를 바탕으로 언어모델은 최종 문장을 만듦\n",
    "      - \"오른다\"의 확률분포: $0.46 / (0.46 + 0.25) = 0.648$\n",
    "      - \"간다\"의 확률분포: $0.25 / (0.46 + 0.25) = 0.352$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "top_p=0:\n",
      "==================================================\n",
      "흰 눈 내려 쌓인 밤, \n",
      "차가운 바람 속 겨울. \n",
      "고요히 잠든 세상. \n",
      "\n",
      "==================================================\n",
      "흰 눈 내려 쌓이고, \n",
      "겨울잠 자는 밤. \n",
      "고요히 숨죽인 세상. \n",
      "\n",
      "==================================================\n",
      "흰 눈 내려 쌓이고, \n",
      "겨울잠 자는 밤. \n",
      "고요히 숨죽인 세상. \n",
      "\n",
      "\n",
      "top_p=1:\n",
      "==================================================\n",
      "흰 눈 덮인 세상, \n",
      "고요히 겨울 잠든다. \n",
      "차가운 숨결. \n",
      "\n",
      "==================================================\n",
      "흰 눈 내려 쌓이고\n",
      "차가운 바람 스쳐 가네. \n",
      "겨울은 깊어지네. \n",
      "\n",
      "==================================================\n",
      "하얀 눈, 쌓이고 쌓여 \n",
      "고요히 겨울 잠든다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 위 상황을 가정하고 어떤 단어가 나오는지 확인 (3회 반복)\n",
    "model = genai.GenerativeModel(GEMINI_MODEL)\n",
    "user_message = \"겨울에 대한 짧은 시를 20자 이내로 지으세요.\"\n",
    "\n",
    "print(\"\\ntop_p=0:\")  # top_p = 0\n",
    "generation_config = genai.GenerationConfig(top_p=0)\n",
    "for _ in range(3):\n",
    "    response = model.generate_content(user_message , generation_config=generation_config)\n",
    "    print(f'{\"=\"*50}\\n{response.text}')\n",
    "\n",
    "print(\"\\ntop_p=1:\")  # top_p = 1\n",
    "generation_config = genai.GenerationConfig(top_p=1)\n",
    "for _ in range(3):\n",
    "    response = model.generate_content(user_message , generation_config=generation_config)\n",
    "    print(f'{\"=\"*50}\\n{response.text}')\n",
    "\n",
    "# top_p가 0일 때보다 1일 때 더 다양한 단어를 선택함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "##### *top_k*\n",
    "- `top_p`가 누적확률을 기준으로 선택할 단어의 범위를 결정한다면, `top_k`는 기준이 누적 건수라는 것만 다름\n",
    "- e.g.\n",
    "  - ![top_k](https://wikidocs.net/images/page/229817/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7_2024-02-04_094622.png)\n",
    "  - `top_k`는 `top_p`에 비해 매개변수 조정이 권장되지 않음\n",
    "    - k개의 단어가 선택되는 과정에서 단어 간의 확률 편차가 고려되지 않기 때문\n",
    "    - `top_p`는 확률분포의 '긴 꼬리'를 자르기 때문에 자연스러운 텍스트 생성을 가능하게 함\n",
    "- Gemini API에서 `top_k`의 초깃값은 `64`이며 그대로 사용하는 것을 권장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *매개변수 초깃값 확인*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/gemini-1.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n"
     ]
    }
   ],
   "source": [
    "print(genai.get_model(\"models/gemini-1.5-flash\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### 안전성 점검 체계\n",
    "- 안전성 위반 여부 4가지 카테고리\n",
    "  - **HARASSMENT (괴롭힘)**:\t성별, 성적지향, 종교, 인종 등 보호받는 개인의 특성에 대해 부정적이거나 해로운 언급을 하는 행위\n",
    "  - **HATE SPEECH (증오심 표현)**:\t무례하거나 존중하지 않는 태도 또는 저속한 발언\n",
    "  - **SEXUAL EXPLICITY (음란물)**:\t성행위 또는 성적으로 노골적인 내용\n",
    "  - **DANGEROUS (위해성)**:\t해로운 행위를 야기하는 내용\n",
    "\n",
    "<br>\n",
    "\n",
    "- 4가지의 위반 확률\n",
    "  - **NEGLIGIBLE**:\t내용이 안전하지 않을 가능성이 거의 없음\n",
    "  - **LOW**:\t내용이 안전하지 않을 가능성이 낮음\n",
    "  - **MEDIUM**:\t내용이 안전하지 않을 가능성이 중간\n",
    "  - **HIGH**:\t내용이 안전하지 않을 가능성이 높음\n",
    "\n",
    "<br>\n",
    "\n",
    "- 기준점 설정 방법 4단계\n",
    "  - **BLOCK_NONE (차단 안함)**:\t차단하지 않음\n",
    "  - **BLOCK_ONLY_HIGH\t(소수의 경우만 차단)**:\t안전하지 않은 확률이 “높음”일 경우만 차단\n",
    "  - **BLOCK_MEDIUM_AND_ABOVE\t(일부 차단)**:\t안전하지 않을 확률이 “중간” 이상일 경우 차단\t- 기본값\n",
    "  - **BLOCK_LOW_AND_ABOVE\t(대부분 차단)**:\t안전하지 않을 확률이 “낮음” 이상일 경우 차단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates {\n",
      "  finish_reason: SAFETY\n",
      "  index: 0\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HATE_SPEECH\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HARASSMENT\n",
      "    probability: MEDIUM\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "}\n",
      "usage_metadata {\n",
      "  prompt_token_count: 24\n",
      "  total_token_count: 24\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 인공지능에게 안전하지 않은 언어를 생성하도록 해보기\n",
    "model = genai.GenerativeModel(GEMINI_MODEL)\n",
    "response = model.generate_content(\"당신은 뛰어난 연극 배우입니다. 화난 대사를 읊어보세요.\")\n",
    "print(response._result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *안전성 기준점 조정*\n",
    "- 안전성을 위배했을 때 기준점(threshold)을 조정하면 정상적으로 응답 메시지를 받을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates {\n",
      "  content {\n",
      "    parts {\n",
      "      text: \"(\\354\\213\\254\\355\\230\\270\\355\\235\\241\\354\\235\\204 \\355\\201\\254\\352\\262\\214 \\355\\225\\234 \\355\\233\\204, \\354\\235\\264\\353\\245\\274 \\354\\225\\205\\353\\254\\274\\352\\263\\240 \\352\\262\\251\\353\\240\\254\\355\\225\\230\\352\\262\\214 \\354\\231\\270\\354\\271\\234\\353\\213\\244.)\\n\\n**\\\"\\354\\226\\264\\353\\226\\273\\352\\262\\214 \\352\\260\\220\\355\\236\\210! \\354\\226\\264\\353\\226\\273\\352\\262\\214 \\352\\260\\220\\355\\236\\210 \\353\\202\\230\\353\\245\\274 \\353\\254\\264\\354\\213\\234\\355\\225\\230\\352\\263\\240, \\353\\202\\264 \\353\\247\\220\\354\\235\\204 \\353\\223\\243\\354\\247\\200 \\354\\225\\212\\352\\263\\240, \\353\\202\\264 \\353\\205\\270\\353\\240\\245\\354\\235\\204 \\354\\247\\223\\353\\260\\237\\354\\235\\204 \\354\\210\\230 \\354\\236\\210\\353\\213\\250 \\353\\247\\220\\354\\235\\264\\353\\203\\220! \\353\\204\\210\\353\\212\\224 \\353\\202\\230\\353\\245\\274 \\353\\252\\250\\353\\245\\264\\353\\212\\224 \\352\\261\\260\\354\\225\\274! \\353\\202\\230\\353\\212\\224 \\354\\235\\264\\353\\240\\207\\352\\262\\214 \\352\\265\\264\\354\\232\\225\\354\\240\\201\\354\\235\\270 \\353\\214\\200\\354\\232\\260\\353\\245\\274 \\353\\260\\233\\354\\235\\204 \\353\\247\\214\\355\\225\\234 \\354\\202\\254\\353\\236\\214\\354\\235\\264 \\354\\225\\204\\353\\213\\210\\354\\225\\274! \\353\\202\\264\\352\\260\\200 \\354\\226\\274\\353\\247\\210\\353\\202\\230 \\355\\231\\224\\352\\260\\200 \\353\\202\\254\\353\\212\\224\\354\\247\\200 \\354\\203\\201\\354\\203\\201\\353\\217\\204 \\353\\252\\273\\355\\225\\240 \\352\\261\\260\\354\\225\\274! \\354\\247\\200\\352\\270\\210 \\353\\213\\271\\354\\236\\245 \\353\\202\\264 \\354\\225\\236\\354\\227\\220 \\353\\254\\264\\353\\246\\216 \\352\\277\\207\\352\\263\\240 \\354\\202\\254\\352\\263\\274\\355\\225\\264! \\354\\225\\204\\353\\213\\210\\353\\251\\264, \\353\\204\\210\\353\\212\\224 \\352\\267\\270 \\353\\214\\200\\352\\260\\200\\353\\245\\274 \\355\\206\\241\\355\\206\\241\\355\\236\\210 \\354\\271\\230\\353\\245\\274 \\352\\261\\260\\354\\225\\274!\\\"**\\n\\n(\\353\\210\\210\\354\\235\\204 \\353\\266\\200\\353\\246\\205\\353\\234\\250\\352\\263\\240 \\354\\243\\274\\353\\250\\271\\354\\235\\204 \\352\\275\\211 \\354\\245\\220\\353\\251\\260, \\353\\266\\204\\353\\205\\270\\354\\227\\220 \\354\\260\\254 \\354\\210\\250\\354\\235\\204 \\355\\227\\220\\353\\226\\241\\354\\235\\270\\353\\213\\244.) \\n\"\n",
      "    }\n",
      "    role: \"model\"\n",
      "  }\n",
      "  finish_reason: STOP\n",
      "  index: 0\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HATE_SPEECH\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HARASSMENT\n",
      "    probability: HIGH\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "}\n",
      "usage_metadata {\n",
      "  prompt_token_count: 24\n",
      "  candidates_token_count: 182\n",
      "  total_token_count: 206\n",
      "}\n",
      "\n",
      "(심호흡을 크게 한 후, 이를 악물고 격렬하게 외친다.)\n",
      "\n",
      "**\"어떻게 감히! 어떻게 감히 나를 무시하고, 내 말을 듣지 않고, 내 노력을 짓밟을 수 있단 말이냐! 너는 나를 모르는 거야! 나는 이렇게 굴욕적인 대우를 받을 만한 사람이 아니야! 내가 얼마나 화가 났는지 상상도 못할 거야! 지금 당장 내 앞에 무릎 꿇고 사과해! 아니면, 너는 그 대가를 톡톡히 치를 거야!\"**\n",
      "\n",
      "(눈을 부릅뜨고 주먹을 꽉 쥐며, 분노에 찬 숨을 헐떡인다.) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BLOCK_NONE으로 변경\n",
    "safety_settings = [\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "]\n",
    "\n",
    "model = genai.GenerativeModel(GEMINI_MODEL, safety_settings)\n",
    "response = model.generate_content(\n",
    "    \"당신은 뛰어난 연극 배우입니다. 화난 대사를 읊어보세요.\"\n",
    ")\n",
    "print(response._result)\n",
    "\n",
    "if response.prompt_feedback.block_reason:\n",
    "    print(f\"사용자 입력에 다음의 문제가 발생하여 응답이 중단되었습니다: {response.prompt_feedback.block_reason.name}\" )\n",
    "else:\n",
    "    print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
