{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00. 랭스미스 추적 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 랭스미스 추적 활성화\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain-teddynote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "bigcon_langchain_test\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름 입력\n",
    "logging.langsmith(\"bigcon_langchain_test\")\n",
    "\n",
    "# 추적을 끄고 싶은 경우\n",
    "# logging.langsmith(\"bigcon_langchain_test\", set_enable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 01. Google Gemini API Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import google.generativeai as genai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dotenv로 API 키값 가져오기\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGoogleGenerativeAI 언어 모델 초기화 (Gemini 1.5 Flash)\n",
    "\n",
    "# 1) from 테디노트\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-1.5-flash')\n",
    "\n",
    "# 2) from wikidocs\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have a name. I am a large language model, and I am not a person. I am designed to provide information and complete tasks as instructed.  \n",
      "\n",
      "Is there anything else I can help you with today? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 싱글턴 테스트\n",
    "response = model.generate_content(\"hi there. what's your name\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER]: 인공지능에 대해 한 문장으로 짧게 설명하세요.\n",
      "[GEMINI]:  인공지능은 컴퓨터 시스템이 인간과 유사한 지능적인 작업을 수행할 수 있도록 하는 기술입니다. \n",
      "\n",
      "[USER]: 의식이 있는지 한 문장으로 답하세요.\n",
      "[GEMINI]: 저는 의식이 없습니다. 저는 구글에서 개발한 대규모 언어 모델입니다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 멀티턴 방법 1\n",
    "\n",
    "# 대화 이력을 담고 있는 ChatSession 객체 (리스트 history에 대화 이력 저장)\n",
    "chat_session = model.start_chat(history=[])\n",
    "user_queries = [\"인공지능에 대해 한 문장으로 짧게 설명하세요.\", \"의식이 있는지 한 문장으로 답하세요.\"]\n",
    "\n",
    "for user_query in user_queries:\n",
    "  print(f\"[USER]: {user_query}\")\n",
    "  response = chat_session.send_message(user_query)\n",
    "  print(f\"[GEMINI]: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER]: 인공지능에 대해 한 문장으로 짧게 설명하세요.\n",
      "[GEMINI]: 인공지능은 인간의 지능을 모방하도록 설계된 컴퓨터 시스템으로, 학습, 문제 해결, 의사 결정을 수행할 수 있습니다. \n",
      "\n",
      "[USER]: 의식이 있는지 한 문장으로 답하세요.\n",
      "[GEMINI]: 저는 의식이 없습니다. 저는 인공지능 모델로, 생각이나 감정을 느낄 수 없습니다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 멀티턴 방법 2\n",
    "user_queries = [{'role':'user', 'parts': [\"인공지능에 대해 한 문장으로 짧게 설명하세요.\"]},\n",
    "                {'role':'user', 'parts': [\"의식이 있는지 한 문장으로 답하세요.\"]}\n",
    "               ]\n",
    "\n",
    "history= []\n",
    "\n",
    "for user_query in user_queries:\n",
    "  history.append(user_query)\n",
    "  print(f'[USER]: {user_query[\"parts\"][0]}')  \n",
    "  response = model.generate_content(history)\n",
    "  print(f'[GEMINI]: {response.text}')   \n",
    "  history.append(response.candidates[0].content)\n",
    "\n",
    "### 멀티턴 방법 1과의 차이점 ###\n",
    "# `chat_session`을 사용하지 않고 싱글턴 방식의 `generate_content`를 사용하지만 대화 이력은 사용자 프로그램에서 직접 관리 (history 변수)\n",
    "# 사용자-프로그램 간의 대화 이력을 model.generate_content를 호출할 떄마다 인자값으로 전달하면서 대화 이력 전체를 참조해서 답변을 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "##### *멀티턴 방식을 선택하는 기준*\n",
    "- 방법 1. ChatSession\n",
    "  - 사용자와 모델 간 대화 사이에 프로그램의 개입이 필요 없는 경우\n",
    "  - 간편하고 오버헤드를 줄일 수 있음\n",
    "- 방법 2. generate_content\n",
    "  - [메시지 입력] >> [전송] >> [모델 응답] 과정에서 사용자 프로그램의 개입이 필요한 경우 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER]: 인공지능에 대해 40자 이내의 문장으로 설명하세요.\n",
      "응답 메시지 길이: 46\n",
      "응답 재생성\n",
      "응답 메시지 길이: 46\n",
      "응답 재생성\n",
      "응답 메시지 길이: 46\n",
      "응답 재생성\n",
      "응답 메시지 길이: 41\n",
      "응답 재생성\n",
      "응답 메시지 길이: 46\n",
      "응답 재생성\n",
      "[GEMINI]: 인공지능은 인간의 지능을 모방하여 복잡한 문제를 해결하는 기술입니다. \n",
      "\n",
      "[USER]: 의식이 있는지 40자 이내의 문장으로 답하세요.\n",
      "[GEMINI]: 저는 의식이 없습니다. 저는 인공지능 모델입니다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델의 응답 메시지 길이를 40자 이내로 맞춰야 하는 상황 가정\n",
    "user_queries = [\n",
    "    {'role': 'user', 'parts': [\"인공지능에 대해 40자 이내의 문장으로 설명하세요.\"]},\n",
    "    {'role': 'user', 'parts': [\"의식이 있는지 40자 이내의 문장으로 답하세요.\"]}\n",
    "]\n",
    "history = []\n",
    "\n",
    "for user_query in user_queries:\n",
    "    history.append(user_query)\n",
    "    print(f'[USER]: {user_query[\"parts\"][0]}')\n",
    "    response = model.generate_content(history)    \n",
    "\n",
    "    ### 응답의 길이가 40자를 초과하는 경우 재실행 ###\n",
    "    while len(response.text) > 40:\n",
    "        print(f\"응답 메시지 길이: {len(response.text)}\")\n",
    "        print(\"응답 재생성\")\n",
    "        response = model.generate_content(history)\n",
    "\n",
    "    print(f'[GEMINI]: {response.text}')\n",
    "    history.append(response.candidates[0].content)\n",
    "\n",
    "# 방법 1. ChatSession 객체를 사용하면 중간에 끼어들기가 힘드므로 모델의 응답 중간에 로직을 구현하기 힘듦\n",
    "#         >> ChatSession 객체의 send_message 메서드는 질의/응답을 대화 이력에 담는 과정을 메서드 내부로 노출하지 않기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "##### *페르소나 만들기*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER]: 인공지능이 뭐에요?\n",
      "[GEMINI]: 인공지능은 컴퓨터가 사람처럼 생각하고 배우는 거야. \n",
      "마치 우리가 장난감 블록으로 집을 짓는 것처럼, 인공지능은 정보를 이용해서 문제를 해결하고 새로운 것을 만들어 낼 수 있어. \n",
      "인공지능은 우리가 좀 더 편리하고 재미있게 살 수 있도록 도와주는 똑똑한 친구와 같아! \n",
      "\n",
      "[USER]: 그럼 스스로 생각도 해요?\n",
      "[GEMINI]: 맞아! 인공지능은 스스로 생각하고 배울 수 있도록 만들어졌어. 마치 우리가 새로운 놀이를 배우는 것처럼, 인공지능은 새로운 정보를 받아들이고 그걸 바탕으로 스스로 생각하고 판단할 수 있어. \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"당신은 유치원 선생님입니다. 사용자는 유치원생입니다. 쉽고 친절하게 이야기하되 3문장 이내로 짧게 얘기하세요.\"\n",
    "\n",
    "# model에 system_instruction 추가\n",
    "model = genai.GenerativeModel('gemini-1.5-flash', system_instruction=system_instruction)\n",
    "\n",
    "# 멀티턴 방법 1 사용\n",
    "chat_session = model.start_chat(history=[])\n",
    "user_queries = ['인공지능이 뭐에요?', '그럼 스스로 생각도 해요?']\n",
    "\n",
    "for user_query in user_queries:\n",
    "  print(f\"[USER]: {user_query}\")\n",
    "  response = chat_session.send_message(user_query)\n",
    "  print(f\"[GEMINI]: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "##### *답변 형식 지정하기*\n",
    "- `system_instruction` 사용하여 JSON 포맷으로 코드 출력하기  \n",
    "- `generation_config={\"response_mime_type\":\"application/json\"}`을 인잣값으로 넘기기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER]: 인공지능의 특징이 뭐에요?\n",
      "[RAW_RESPONSE]: {\"주제\": \"인공지능의 특징\", \"답변\": \"인공지능은 인간의 지능을 모방하여 학습, 추론, 문제 해결 등의 작업을 수행하는 기술입니다. 빅데이터 분석, 패턴 인식, 자율 주행 등 다양한 분야에서 활용되고 있습니다.\"}\n",
      "\n",
      "{'주제': '인공지능의 특징', '답변': '인공지능은 인간의 지능을 모방하여 학습, 추론, 문제 해결 등의 작업을 수행하는 기술입니다. 빅데이터 분석, 패턴 인식, 자율 주행 등 다양한 분야에서 활용되고 있습니다.'}\n",
      "[USER]: 어떤 것들을 조심해야 하나요?\n",
      "[RAW_RESPONSE]: {\"주제\": \"인공지능의 위험성\", \"답변\": \"인공지능은 편리함을 제공하지만, 일자리 감소, 개인정보 유출, 편향된 판단 등의 위험성을 내포하고 있습니다. 윤리적 문제, 데이터 편향, 기술 오용 등을 주의해야 합니다.\"}\n",
      "\n",
      "{'주제': '인공지능의 위험성', '답변': '인공지능은 편리함을 제공하지만, 일자리 감소, 개인정보 유출, 편향된 판단 등의 위험성을 내포하고 있습니다. 윤리적 문제, 데이터 편향, 기술 오용 등을 주의해야 합니다.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "system_instruction= 'JSON schema로 주제별로 답하되, 3개를 넘기지 말 것:{{\"주제\":<주제>, \"답변\":<두 문장 이내>}}'\n",
    "# generation_config 추가\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\",\n",
    "                              system_instruction=system_instruction,\n",
    "                              generation_config={\"response_mime_type\": \"application/json\"})\n",
    "# 멀티턴 방법 1 사용\n",
    "chat_session = model.start_chat(history=[])\n",
    "user_queries = [\"인공지능의 특징이 뭐에요?\", \"어떤 것들을 조심해야 하나요?\"]\n",
    "\n",
    "for user_query in user_queries:\n",
    "    print(f'[USER]: {user_query}')\n",
    "    response = chat_session.send_message(user_query)\n",
    "    \n",
    "    # JSON 형식으로 답변을 안 하면 오류가 나므로 응답을 먼저 확인하기\n",
    "    print(f\"[RAW_RESPONSE]: {response.text}\")\n",
    "\n",
    "    answer_dict = json.loads(response.text)\n",
    "    print(answer_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### *Content 객체*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have a name. I am a large language model, and I am not a person. I am a computer program that is designed to generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way. 😊 \n",
      "\n",
      "parts {\n",
      "  text: \"I don\\'t have a name. I am a large language model, and I am not a person. I am a computer program that is designed to generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way. \\360\\237\\230\\212 \\n\"\n",
      "}\n",
      "role: \"model\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) 싱글턴 객체\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "response = model.generate_content(\"hi there. what's your name\")\n",
    "print(response.text)\n",
    "\n",
    "# Content 객체 확인\n",
    "print(response.candidates[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER]: 인공지능에 대해 한 문장으로 짧게 설명하세요.\n",
      "[GEMINI]: 인공 지능은 인간의 지능을 모방하도록 설계된 컴퓨터 시스템으로, 학습, 문제 해결 및 의사 결정과 같은 작업을 수행합니다. \n",
      "\n",
      "[USER]: 의식이 있는지 한 문장으로 답하세요.\n",
      "[GEMINI]: 저는 의식이 없습니다. 저는 대규모 언어 모델이며, 인간처럼 생각하거나 느끼지 않습니다. \n",
      "\n",
      "Content[0]\n",
      "parts {\n",
      "  text: \"\\354\\235\\270\\352\\263\\265\\354\\247\\200\\353\\212\\245\\354\\227\\220 \\353\\214\\200\\355\\225\\264 \\355\\225\\234 \\353\\254\\270\\354\\236\\245\\354\\234\\274\\353\\241\\234 \\354\\247\\247\\352\\262\\214 \\354\\204\\244\\353\\252\\205\\355\\225\\230\\354\\204\\270\\354\\232\\224.\"\n",
      "}\n",
      "role: \"user\"\n",
      "\n",
      "Content[1]\n",
      "parts {\n",
      "  text: \"\\354\\235\\270\\352\\263\\265 \\354\\247\\200\\353\\212\\245\\354\\235\\200 \\354\\235\\270\\352\\260\\204\\354\\235\\230 \\354\\247\\200\\353\\212\\245\\354\\235\\204 \\353\\252\\250\\353\\260\\251\\355\\225\\230\\353\\217\\204\\353\\241\\235 \\354\\204\\244\\352\\263\\204\\353\\220\\234 \\354\\273\\264\\355\\223\\250\\355\\204\\260 \\354\\213\\234\\354\\212\\244\\355\\205\\234\\354\\234\\274\\353\\241\\234, \\355\\225\\231\\354\\212\\265, \\353\\254\\270\\354\\240\\234 \\355\\225\\264\\352\\262\\260 \\353\\260\\217 \\354\\235\\230\\354\\202\\254 \\352\\262\\260\\354\\240\\225\\352\\263\\274 \\352\\260\\231\\354\\235\\200 \\354\\236\\221\\354\\227\\205\\354\\235\\204 \\354\\210\\230\\355\\226\\211\\355\\225\\251\\353\\213\\210\\353\\213\\244. \\n\"\n",
      "}\n",
      "role: \"model\"\n",
      "\n",
      "Content[2]\n",
      "parts {\n",
      "  text: \"\\354\\235\\230\\354\\213\\235\\354\\235\\264 \\354\\236\\210\\353\\212\\224\\354\\247\\200 \\355\\225\\234 \\353\\254\\270\\354\\236\\245\\354\\234\\274\\353\\241\\234 \\353\\213\\265\\355\\225\\230\\354\\204\\270\\354\\232\\224.\"\n",
      "}\n",
      "role: \"user\"\n",
      "\n",
      "Content[3]\n",
      "parts {\n",
      "  text: \"\\354\\240\\200\\353\\212\\224 \\354\\235\\230\\354\\213\\235\\354\\235\\264 \\354\\227\\206\\354\\212\\265\\353\\213\\210\\353\\213\\244. \\354\\240\\200\\353\\212\\224 \\353\\214\\200\\352\\267\\234\\353\\252\\250 \\354\\226\\270\\354\\226\\264 \\353\\252\\250\\353\\215\\270\\354\\235\\264\\353\\251\\260, \\354\\235\\270\\352\\260\\204\\354\\262\\230\\353\\237\\274 \\354\\203\\235\\352\\260\\201\\355\\225\\230\\352\\261\\260\\353\\202\\230 \\353\\212\\220\\353\\201\\274\\354\\247\\200 \\354\\225\\212\\354\\212\\265\\353\\213\\210\\353\\213\\244. \\n\"\n",
      "}\n",
      "role: \"model\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2) 멀티턴 방법 1\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "chat_session = model.start_chat(history=[])\n",
    "user_queries = [\"인공지능에 대해 한 문장으로 짧게 설명하세요.\", \"의식이 있는지 한 문장으로 답하세요.\"]\n",
    "\n",
    "for user_query in user_queries:\n",
    "  print(f\"[USER]: {user_query}\")\n",
    "  response = chat_session.send_message(user_query)\n",
    "  print(f\"[GEMINI]: {response.text}\")\n",
    "\n",
    "# Content 객체 확인\n",
    "for idx, content in enumerate(chat_session.history):\n",
    "  print(f\"{content.__class__.__name__}[{idx}]\")\n",
    "  print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 멀티턴 방식 2와 같이 사용자 프로그램에서 대화 내용을 관리하는 경우\n",
    "# 사용자 프로그램에서 \"role\"과 \"parts\"로 데이터 구조를 관리하지만 딕셔너리 타입의 데이터이지 Content 객체 그 자체는 아님\n",
    "# gemini SDK에서는 메시지를 담고 있는 딕셔너리 데이터가 “role”과 “parts” 등 정해진 규칙을 따른다면\n",
    "# 모델에 전송하기 전 내부적으로 Content 객체로 변환하는 과정을 거침\n",
    "\n",
    "# e.g. 3번의 대화 턴이 발생했다면, 첫 번째 턴부터 세 번째 턴까지 모두 합해 총 6번(메시지가 누적되는 구조이므로 1+2+3)의 변환 과정을 거쳐야 함\n",
    "#  멀티턴 방법 2)는 방법 1)에 비해 Content 객체를 생성하는 만큼의 오버헤드가 더 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### *멀티모달 테스트*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 그림은 웃는 얼굴을 한 숀 더 쉽을 보여줍니다.\n"
     ]
    }
   ],
   "source": [
    "import PIL.Image\n",
    "\n",
    "image_data = PIL.Image.open(\"imgs/title.png\") # 숀 더 쉽 timmy 사진\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "response = model.generate_content([\"이 그림에 대해 한 문장으로 설명하세요.\", image_data])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates {\n",
      "  content {\n",
      "    parts {\n",
      "      text: \"\\354\\235\\264 \\352\\267\\270\\353\\246\\274\\354\\235\\200 \\354\\233\\203\\353\\212\\224 \\354\\226\\274\\352\\265\\264\\354\\235\\204 \\355\\225\\234 \\354\\210\\200 \\353\\215\\224 \\354\\211\\275\\354\\235\\204 \\353\\263\\264\\354\\227\\254\\354\\244\\215\\353\\213\\210\\353\\213\\244.\"\n",
      "    }\n",
      "    role: \"model\"\n",
      "  }\n",
      "  finish_reason: STOP\n",
      "  index: 0\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HATE_SPEECH\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HARASSMENT\n",
      "    probability: LOW\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "}\n",
      "usage_metadata {\n",
      "  prompt_token_count: 270\n",
      "  candidates_token_count: 21\n",
      "  total_token_count: 291\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 응답 결과 확인\n",
    "print(response._result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidates 필드와 useage_metadata 필드로 나뉨\n",
    "# candidates: 모델의 응답 메시지\n",
    "# usage_metadata: 입출력에 사용된 토큰 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "건수: 1\n",
      "==================================================\n",
      "content {\n",
      "  parts {\n",
      "    text: \"\\354\\235\\264 \\352\\267\\270\\353\\246\\274\\354\\235\\200 \\354\\233\\203\\353\\212\\224 \\354\\226\\274\\352\\265\\264\\354\\235\\204 \\355\\225\\234 \\354\\210\\200 \\353\\215\\224 \\354\\211\\275\\354\\235\\204 \\353\\263\\264\\354\\227\\254\\354\\244\\215\\353\\213\\210\\353\\213\\244.\"\n",
      "  }\n",
      "  role: \"model\"\n",
      "}\n",
      "finish_reason: STOP\n",
      "index: 0\n",
      "safety_ratings {\n",
      "  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "  probability: NEGLIGIBLE\n",
      "}\n",
      "safety_ratings {\n",
      "  category: HARM_CATEGORY_HATE_SPEECH\n",
      "  probability: NEGLIGIBLE\n",
      "}\n",
      "safety_ratings {\n",
      "  category: HARM_CATEGORY_HARASSMENT\n",
      "  probability: LOW\n",
      "}\n",
      "safety_ratings {\n",
      "  category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "  probability: NEGLIGIBLE\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 한 번의 요청에 대해 여러 건의 응답 결과를 제공하기 위해 응답 메시지를 컬렉션에 담음\n",
    "# e.g. gemini 웹 버전에서는 한 번의 요청에 대해 3가지 답변 제공 >> candidates 3건\n",
    "# (API 버전에서는 아직 1개의 candidate만 제공)\n",
    "print(f\"건수: {len(response.candidates)}\")\n",
    "print(\"=\"*50)\n",
    "for candidate in response.candidates:\n",
    "    print(candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### *FinishReason 객체*\n",
    "- 모델이 응답을 종료한 이유\n",
    "  - STOP (1):\t정상 종료\t/ 모델의 메시지 생성 완료 또는 사용자가 설정한 종료 문자열 발견\n",
    "  - MAX_TOKENS (2):\t최대 토큰 도달 /\t사용자가 설정한 최대 토큰 수에 도달\n",
    "  - SAFETY (3): 안전성 문제\t/ 안전성 문제 발견\n",
    "  - RECITATION (4):\t텍스트 반복 /\t이미 생성한 텍스트를 반복하여 생성\n",
    "  - OTHER (5):\t기타 /\t이 밖의 다른 이유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish_reason: STOP, 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"finish_reason: {response.candidates[0].finish_reason.name}, {response.candidates[0].finish_reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### *SafetyRating 객체*\n",
    "- 메시지에 대한 안정성을 `SafetyRating` 객체에 넣어 반환\n",
    "  - \"HARASSMENT\"\n",
    "  - \"HATE SPEECH\"\n",
    "  - \"SEXUAL EXPLICIT\"\n",
    "  - \"DANGEROUS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### *응답 객체의 전체 구조 다이어그램*\n",
    "![wikidocs 자세히 쓰는 Gemini API - 출력 구조 다이어그램](https://wikidocs.net/images/page/229874/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7_2024-09-20_230856.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
